{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140002 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the main folder path\n",
    "main_folder_path = 'dataset/Train'\n",
    "\n",
    "# Use image_dataset_from_directory to load images efficiently\n",
    "batch_size = 32  # Adjust batch size as per your system's memory capacity\n",
    "img_height, img_width = 224, 224  # Set the desired image size\n",
    "\n",
    "# Load the dataset\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    main_folder_path,\n",
    "    labels='inferred',  # Automatically infer labels based on subfolder names\n",
    "    label_mode='int',   # Use 'int' for integer labels (0 or 1)\n",
    "    class_names=['Fake', 'Real'],  # Ensure 'Fake' maps to 0 and 'Real' maps to 1\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Optionally, convert the dataset to numpy arrays if needed\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for img_batch, label_batch in dataset:\n",
    "    images.append(img_batch.numpy())\n",
    "    labels.append(label_batch.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = np.concatenate(images, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "print(f\"Loaded {len(images)} images with labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190335 190335\n",
      "(190335, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(images, labels, test_size=0.1, random_state=42) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, random_state=42) \n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset\\Test\\Fake\\fake_0.jpg</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset\\Test\\Fake\\fake_1.jpg</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset\\Test\\Fake\\fake_10.jpg</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset\\Test\\Fake\\fake_100.jpg</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset\\Test\\Fake\\fake_1000.jpg</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image label\n",
       "0     dataset\\Test\\Fake\\fake_0.jpg  Fake\n",
       "1     dataset\\Test\\Fake\\fake_1.jpg  Fake\n",
       "2    dataset\\Test\\Fake\\fake_10.jpg  Fake\n",
       "3   dataset\\Test\\Fake\\fake_100.jpg  Fake\n",
       "4  dataset\\Test\\Fake\\fake_1000.jpg  Fake"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_resnet_model(input_shape=(224, 224, 3)):\n",
    "    model = models.Sequential([\n",
    "        ResNet50(weights='imagenet', include_top=False, input_shape=input_shape),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001))\n",
    "    ])\n",
    "    model.layers[0].trainable = False  # Freeze base layers\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the ResNet model\n",
    "model = create_resnet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fake', 'Real'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "y_pred_proba_test = model.predict(X_test).ravel()\n",
    "y_pred = (y_pred_proba_test >= 0.5).astype(int)\n",
    "cm_test = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm_test, annot=True,fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix (Test Accuracy: {test_accuracy:.4f})')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
